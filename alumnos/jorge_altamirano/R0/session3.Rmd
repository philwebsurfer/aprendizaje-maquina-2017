---
title: "Tarea 3: 04/09/2017"
output: html_notebook
---
```{r imports, warning=FALSE}
library(readr)
library(dplyr)
library(tidyr)
```

# Tarea 3: Regresión Logística 04/09/2017

Identificación de dígitos "altos": 5, 6, 7, 8, 9.

## Entrenamiento 

```{r learn chunk}
digitos_entrena <- read_csv('../../../datos/zip-train.csv')
digitos_prueba <- read_csv('../../../datos/zip-test.csv')
names(digitos_entrena)[1] <- 'digito'
names(digitos_entrena)[2:257] <- paste0('pixel_', 1:256)
names(digitos_prueba)[1] <- 'digito'
names(digitos_prueba)[2:257] <- paste0('pixel_', 1:256)
dim(digitos_entrena)
```

```{r}
table(digitos_entrena$digito)
```
# Graficación
```{r digits graph}
graficar_digitos <- function(d_frame){
  matriz_digitos <- lapply(1:nrow(d_frame), function(i){ 
    	matrix(as.numeric(d_frame[i, 257:2]), 16, 16)[16:1, ]
    })
	image(Reduce("rbind", matriz_digitos), 
    col = terrain.colors(30), axes = FALSE)
	text(seq(0,0.9, length.out = nrow(d_frame)) + 0.05, 0.05, label = d_frame$digito, cex = 1.5)
}
graficar_digitos(digitos_entrena[1:5,])
```

##0. División de entrenamiento y prueba; normalización; diferente letra

* _Explica por qué es menos importante normalizar en este caso, pero de todas formas puede ser una buena idea_: 
  Considero que no es tan relevante normalizar dado que se encuentran las muestras en la misma medida o escala. Sin embargo, puede auxiliar en quitar el ruido.
* _En estos datos de entrenamiento, los dígitos en la muestra de prueba son escritos por personas diferentes que los de la muestra de entrenamiento. Explica por qué esto es importante para validar correctamente el modelo._
```{r}
graficar_digitos(digitos_prueba[1:5,])
```
  Debido a que así se tienen más datos y se puede probar en más certeramente en el mundo exterior: donde deseamos ejecutar el modelo
  
## 1. Ajusta con descenso en gradiente un modelo de regresión logística, y compara con la salida de glm para checar tus cálculos.

```{r}
grad_calc <- function(x_ent, y_ent){
  salida_grad <- function(beta){
    p_beta <- h(as.matrix(cbind(1, x_ent)) %*% beta) 
    e <- y_ent - p_beta
    grad_out <- -2*as.numeric(t(cbind(1,x_ent)) %*% e)
    names(grad_out) <- c('Intercept', colnames(x_ent))
    grad_out
  }
  salida_grad
}
descenso <- function(n, z_0, eta, h_deriv){
  z <- matrix(0,n, length(z_0))
  z[1, ] <- z_0
  for(i in 1:(n-1)){
    z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
  }
  z
}

digitos_entrena$y <- as.numeric(digitos_entrena$digito<5)
#normalización
digitos_entrena <- digitos_entrena %>% mutate(id=1:n()) %>% gather(pixel, x, pixel_1:pixel_256) 
# digitos_entrena <- digitos_entrena %>% mutate(pixel = parse_number(pixel))
digitos_entrena <- digitos_entrena %>% ungroup %>% mutate(x_s = (x - mean(x))/sd(x))

#digitos_entrena %>% spread(key = pixel, value = x, fill = TRUE, drop = FALSE)
digitos_entrena 
```

Me aparece este error post normalización al utilizar ```spread```
```{r}
knitr::include_graphics(path = c("error_spread.jpg"))
```

