---
title: "Tarea 2"
output: html_notebook
---

```{r, warnings = FALSE, messages =FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(stringr)
```


```{r}
housing <- read_table('../datos/housing/housing.data', 
                      col_names = FALSE)
lineas <- readLines('../datos/housing/housing.names', n = -1)
lineas
lineas_1 <- str_subset(lineas, '\\s+[0-9]+.\\s')
nombres <- str_match(lineas_1, pattern = '\\s+[0-9]+.\\s(\\w+)')[4:17,2]
names(housing) <- nombres
housing
```

Estas son las funciones que utilizaremos para descenso en gradiente

```{r}
rss_calc <- function(x, y){
  # x es un data.frame o matrix con entradas
  # y es la respuesta
  rss_fun <- function(beta){
    # esta funcion debe devolver rss
    y_hat <- as.matrix(cbind(1,x)) %*% beta
    e <- y - y_hat
    rss <- 0.5*sum(e^2)
    rss
  }
  rss_fun
}


grad_calc <- function(x, y){
  # devuelve una función que calcula el gradiente para 
  # parámetros beta   
  # x es un data.frame o matrix con entradas
  # y es la respuesta
  grad_fun <- function(beta){
      f_beta <- as.matrix(cbind(1, x)) %*% beta
      e <- y - f_beta
      gradiente <- -apply(t(cbind(1,x)) %*% e, 1, sum)
      names(gradiente)[1] <- 'Intercept'
      gradiente
    }
   grad_fun
}


descenso <- function(n, z_0, eta, h_grad){
  # esta función calcula n iteraciones de descenso en gradiente 
  z <- matrix(0,n, length(z_0))
  z[1, ] <- z_0
  for(i in 1:(n-1)){
    z[i+1,] <- z[i,] - eta*h_grad(z[i,])
  }
  z
}
```


### Separamos muestras de entrenamiento y prueba

```{r}
set.seed(923)
housing$unif <- runif(nrow(housing), 0, 1)
housing <- arrange(housing, unif)
housing$id <- 1:nrow(housing)
dat_e <- housing[1:400,]
dat_p <- housing[400:nrow(housing),]
dim(dat_e)
dim(dat_p)
```

### Normalización

```{r}
dat_norm <- housing %>% select(-id, -MEDV, -unif) %>%
  gather(variable, valor, CRIM:LSTAT) %>%
  group_by(variable) %>% summarise(m = mean(valor), s = sd(valor))
dat_norm

normalizar <- function(datos, dat_norm){
  datos_salida <- datos %>% select(-unif) %>%
    gather(variable, valor, CRIM:LSTAT) %>%
    left_join(dat_norm) %>%
    mutate(valor_s = (valor - m)/s) %>%
    select(id, MEDV, variable, valor_s) %>%
    spread(variable, valor_s)
}
dat_e_norm <- normalizar(dat_e, dat_norm)
dat_p_norm <- normalizar(dat_p, dat_norm)
```

### Ajuste

Calculamos las funciones
```{r}
x_ent <- dat_e_norm %>% select(-id, -MEDV)
y_ent <- dat_e_norm$MEDV
rss <- rss_calc(x_ent, y_ent)
grad <- grad_calc(x_ent, y_ent) 
```

Hacemos descenso en gradiente:

```{r}
iteraciones <- descenso(1000, rep(0, ncol(x_ent)+1), 0.0001, grad)
rss_iteraciones <- apply(iteraciones, 1, rss)
plot(rss_iteraciones[500:1000])
```

```{r}
beta <- iteraciones[1000,]
dat_coef <- data_frame(variable = c('Intercept',colnames(x_ent)), beta = beta)
quantile(y_ent)
dat_coef %>% mutate(beta = round(beta, 2)) %>% arrange(desc(abs(beta)))
```

Recordemos que  la media de *MEDV* es `r round(mean(y_ent),1)`. Las
variables LSTAT, RM, DIS, PTRATIO, NOX tienen contribuciones considerables al
predictor (2 mil o más dólares por desviación estándar). Variables
como AGE, ZN, INDUS contribuyen considerablemente menos (200 dólares o menos
por desviación estándar, que es una diferencia poco importante en este
contexto).


Comparamos con *lm* para checar nuestro trabajo:

```{r}
lm(MEDV ~ ., data= dat_e_norm %>% select(-id))
```

Ahora evaluamos con la muestra de prueba:

```{r}
calcular_preds <- function(x, beta){
  cbind(1, as.matrix(x))%*%beta
}
x_pr <- dat_p_norm %>% select(-id, -MEDV)
y_pr <- dat_p_norm$MEDV
preds <- calcular_preds(x_pr, beta)
qplot(x = preds, y = y_pr) + geom_abline(intercept = 0, slope = 1)
error_prueba <- mean((y_pr-preds)^2)
sqrt(error_prueba)
```

Este número podemos interpretarlo en la escala de la variable que queremos predecir
(está en miles de dólares).

También podemos evaluar otro tipo de errores que pueden interpretarse
más fácilmente, por ejemplo, la media del
las diferencias en valores absolutos:

```{r}
mean(abs(y_pr-preds))
```

En promedio, el error absoluto
es de `r round(mean(abs(y_pr-preds)),1)` miles de dólares (recordemos que
el valor medio es alrededor de `r round(mean(y_ent),1)` mil dólares).


### k-vecinos más cercanos

```{r}
library(kknn)
error_pred_vmc <- function(dat_ent, dat_prueba){
  salida <- function(k){
       vmc <- kknn(MEDV ~ ., train = dat_ent,  k = k,
                test = dat_prueba, kernel = 'rectangular')
       sqrt(mean((predict(vmc) - dat_prueba$MEDV)^2))
  }
  salida
}
calc_error <- error_pred_vmc(dat_e_norm, dat_p_norm)
dat_vmc <- data_frame(k = c(1,5,seq(10,200,10)))
dat_vmc <- dat_vmc %>% rowwise %>% mutate(error_prueba = calc_error(k))
dat_vmc
ggplot(dat_vmc, aes(x = k, y = error_prueba)) + geom_line() + geom_point() +
  geom_abline(intercept = sqrt(error_prueba), slope=0, colour='red') +
  annotate('text', x = 150, y = 4.6, label = 'modelo lineal', colour = 'red')
```

